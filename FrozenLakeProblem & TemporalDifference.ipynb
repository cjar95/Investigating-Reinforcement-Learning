{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab08b00",
   "metadata": {},
   "source": [
    "# Sub-Task 6-7 (Frozen Lake Problem, Temporal Difference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e4f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports of the necessary libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bf6569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis code is adapted from:\\n\\nhttps://github.com/alexParmar488/OpenAI-Gym-Environment-Exploration\\n\\nI combined the classes into one file, tweaked the graph output dependent on the agent in question \\n\\nAll credit goes to the author.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code is adapted from:\n",
    "\n",
    "https://github.com/alexParmar488/OpenAI-Gym-Environment-Exploration\n",
    "\n",
    "I combined the classes into one file, tweaked the graph output dependent on the agent in question \n",
    "\n",
    "All credit goes to the author.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba5926",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b3abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Class to be used with Frozen Lake\n",
    "class Random():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    #The training action is any random action from within the environment action space\n",
    "    def action(self, env):\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa772a3",
   "metadata": {},
   "source": [
    "### Q-Learner for Frozen Lake & Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31642c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Learner Class to be used with Frozen Lake & Cart-Pole for comparison\n",
    "class Qlearner():\n",
    "    def __init__(self, parameters):\n",
    "        self.alpha = parameters['alpha']\n",
    "        self.gamma = parameters['gamma']\n",
    "        self.epsilon = parameters['epsilon']\n",
    "        super().__init__()\n",
    "    \n",
    "    #Q_LEARNER METHODS FOR FROZEN LAKE\n",
    "    #Initializes the q-table for the frozen lake \n",
    "    def initialize_frozen_lake_q_table(self, env):\n",
    "        self.q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        \n",
    "    #Method defines the training action for q-learning for the frozen lake\n",
    "    def frozen_lake_training_action(self, env, observation):\n",
    "        self.previous_observation = observation\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return env.action_space.sample() # explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[observation]) # exploit/ use the highest q-value\n",
    "        \n",
    "    #Method evaluates the action for frozen lake\n",
    "    def frozen_lake_evaluation_action(self, observation):\n",
    "        return np.argmax(self.q_table[observation])\n",
    "    \n",
    "    #Updates the previous observation qtable entry with the reward gained,\n",
    "    #Uses the maximum/best future option always\n",
    "    #Updates the q-value\n",
    "    def frozen_lake_update(self, observation, action, reward):\n",
    "        old_value = self.q_table[self.previous_observation, action]\n",
    "        next_max = np.max(self.q_table[observation])\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[self.previous_observation, action] = new_value\n",
    "        \n",
    "    #Q-LEARNER METHODS FOR CART_POLE\n",
    "    #Initializes the q-table for the cart-pole\n",
    "    def initialize_cartpole_q_table(self, env):\n",
    "        obs_space = CARTPOLE_POSITION_BUCKETS * CARTPOLE_VELOCITY_BUCKETS * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        self.q_table = np.zeros([obs_space, env.action_space.n])\n",
    "\n",
    "        #establish weak priors to optimise training - if theta < 0, move left, if theta > 0 move right\n",
    "        for i in range(obs_space):\n",
    "            if (i % (CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS) < (CARTPOLE_THETA_BUCKETS / 2)):\n",
    "                self.q_table[i][0] = 0.1\n",
    "            elif (i % (CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS) >= (CARTPOLE_THETA_BUCKETS / 2)):\n",
    "                self.q_table[i][1] = 0.1\n",
    "                \n",
    "    #Method defines the training action for q-learning for cart-pole\n",
    "    def cartpole_training_action(self, env, observation):\n",
    "        self.previous_observation = observation\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return env.action_space.sample() # explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[self._cartpole_obs_index(observation)]) # exploit\n",
    "        \n",
    "    #Method evaluates the action for cart-pole\n",
    "    def cartpole_evaluation_action(self, observation):\n",
    "        return np.argmax(self.q_table[self._cartpole_obs_index(observation)])\n",
    "\n",
    "    #Updates the previous observation qtable entry with the reward gained,\n",
    "    #Uses the maximum/best future option always\n",
    "    #Updates the q-value\n",
    "    def cartpole_update(self, observation, action, reward):\n",
    "        old_value = self.q_table[self._cartpole_obs_index(self.previous_observation), action]\n",
    "        next_max = np.max(self.q_table[self._cartpole_obs_index(observation)])\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[self._cartpole_obs_index(self.previous_observation), action] = new_value\n",
    "        \n",
    "    #Because cartpole observations are continuous, we have to bucket them and\n",
    "    #Calculate an index for the qtable\n",
    "    def _cartpole_obs_index(self, observation):\n",
    "        position, velocity, theta, theta_velocity = observation\n",
    "\n",
    "        bucketed_position = self._bucket(position, CARTPOLE_POSITION_BUCKETS, CARTPOLE_POSITION_RANGE)\n",
    "        bucketed_velocity = self._bucket(velocity, CARTPOLE_VELOCITY_BUCKETS, CARTPOLE_VELOCITY_RANGE)\n",
    "        bucketed_theta = self._bucket(theta, CARTPOLE_THETA_BUCKETS, CARTPOLE_THETA_RANGE)\n",
    "        bucketed_theta_velocity = self._bucket(theta_velocity, CARTPOLE_THETA_VELOCITY_BUCKETS, CARTPOLE_THETA_VELOCITY_RANGE)\n",
    "\n",
    "        position_index = (bucketed_position - 1) * CARTPOLE_VELOCITY_BUCKETS * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        velocity_index = (bucketed_velocity - 1) * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        theta_index = (bucketed_theta - 1) * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        theta_velocity_index = (bucketed_theta_velocity - 1)\n",
    "\n",
    "        index = position_index + velocity_index + theta_index + theta_velocity_index\n",
    "        return index\n",
    "    \n",
    "    #Calculate bucket number\n",
    "    def _bucket(self, observation, num_buckets, obs_range):\n",
    "        r_min = obs_range[0]\n",
    "        r_max = obs_range[1]\n",
    "        r_range = r_max - r_min\n",
    "        bucket_size = r_range / num_buckets\n",
    "        bucket = math.ceil((observation + r_range / 2) / bucket_size)\n",
    "\n",
    "        # bound\n",
    "        bucket = min(bucket, num_buckets)\n",
    "        bucket = max(bucket, 1)\n",
    "        return bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e2367",
   "metadata": {},
   "source": [
    "### Temporal Difference for Frozen Lake & Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c214f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cartpole bucket hyperparameters\n",
    "CARTPOLE_POSITION_BUCKETS = 2\n",
    "CARTPOLE_POSITION_RANGE = (-2.0, 2.0)\n",
    "CARTPOLE_VELOCITY_BUCKETS = 8\n",
    "CARTPOLE_VELOCITY_RANGE = (-1.2, 1.2)\n",
    "CARTPOLE_THETA_BUCKETS = 16\n",
    "CARTPOLE_THETA_RANGE = (-0.08, 0.08)\n",
    "CARTPOLE_THETA_VELOCITY_BUCKETS = 6\n",
    "CARTPOLE_THETA_VELOCITY_RANGE = (-1.2, 1.2)\n",
    "\n",
    "#Implementation of SARSA\n",
    "#Temporal Difference class to be used with Frozen Lake and Cart-Pole\n",
    "class TDlearner():\n",
    "    def __init__(self, parameters):\n",
    "        self.alpha = parameters['alpha']\n",
    "        self.gamma = parameters['gamma']\n",
    "        self.epsilon = parameters['epsilon']\n",
    "        super().__init__()\n",
    "        \n",
    "    #FROZEN LAKE METHODS\n",
    "    #Initializes frozen lake q-policy\n",
    "    def initialize_frozen_lake_q_policy(self, env):\n",
    "        self.q_policy = np.ones([env.observation_space.n, env.action_space.n])\n",
    "        self.obs_range = env.action_space.n\n",
    "    \n",
    "    #Method defines the frozen lake training action to incur \n",
    "    def frozen_lake_training_action(self, env, observation):\n",
    "        self.previous_observation = observation\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return env.action_space.sample() # explore\n",
    "        else:\n",
    "            # exploit using a probability weighted selection fom future states\n",
    "            # with the existing policy\n",
    "            next_actions = self.q_policy[observation]\n",
    "            next_actions_sum = sum(next_actions)\n",
    "            weighted_actions = [action / next_actions_sum for action in next_actions]\n",
    "            return np.random.choice(np.arange(self.obs_range), p=weighted_actions)\n",
    "   \n",
    "    #Method evaluatues actions incurred\n",
    "    def frozen_lake_evaluation_action(self, observation):\n",
    "        next_actions = self.q_policy[observation]\n",
    "        next_actions_sum = sum(next_actions)\n",
    "        weighted_actions = [action / next_actions_sum for action in next_actions]\n",
    "        return np.random.choice(np.arange(self.obs_range), p=weighted_actions)\n",
    "    \n",
    "    # Method updates the policy with the reward gained, using a probability weighted\n",
    "    # selection fom future states with the existing policy\n",
    "    def frozen_lake_update(self, observation, action, reward):\n",
    "        old_value = self.q_policy[self.previous_observation, action]\n",
    "        next_actions = self.q_policy[observation]\n",
    "        next_actions_sum = sum(next_actions)\n",
    "        weighted_actions = [action / next_actions_sum for action in next_actions]\n",
    "        next_action_score = np.random.choice(next_actions, p=weighted_actions)\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_action_score)\n",
    "        self.q_policy[self.previous_observation, action] = new_value\n",
    "        \n",
    "    #CartPole Methods\n",
    "    #Initializes cart-pole q-policy\n",
    "    def initialize_cartpole_q_policy(self, env):\n",
    "        obs_space = CARTPOLE_POSITION_BUCKETS * CARTPOLE_VELOCITY_BUCKETS * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        self.q_policy = np.ones([obs_space, env.action_space.n])\n",
    "        self.obs_range = env.action_space.n\n",
    "\n",
    "        # establish weak priors to optimise training - if theta < 0, move left, if theta > 0 move right\n",
    "        for i in range(obs_space):\n",
    "            if (i % (CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS) < (CARTPOLE_THETA_BUCKETS / 2)):\n",
    "                self.q_policy[i][0] = 0.1\n",
    "            elif (i % (CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS) >= (CARTPOLE_THETA_BUCKETS / 2)):\n",
    "                self.q_policy[i][1] = 0.1\n",
    "                \n",
    "    #Defines the training action taken by the cart-pole environment \n",
    "    def cartpole_training_action(self, env, observation):\n",
    "        self.previous_observation = observation\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return env.action_space.sample() # explore\n",
    "        else:\n",
    "            # exploit using a probability weighted selection fom future states\n",
    "            # with the existing policy\n",
    "            next_actions = self.q_policy[self._cartpole_obs_index(observation)]\n",
    "            next_actions_sum = sum(next_actions)\n",
    "            weighted_actions = [action / next_actions_sum for action in next_actions]\n",
    "            return np.random.choice(np.arange(self.obs_range), p=weighted_actions)\n",
    "        \n",
    "    #Method evaluatues actions incurred\n",
    "    def cartpole_evaluation_action(self, observation):\n",
    "        next_actions = self.q_policy[self._cartpole_obs_index(observation)]\n",
    "        next_actions_sum = sum(next_actions)\n",
    "        weighted_actions = [action / next_actions_sum for action in next_actions]\n",
    "        return np.random.choice(np.arange(self.obs_range), p=weighted_actions)\n",
    "    \n",
    "    #updates the policy with the reward gained, using a probability weighted\n",
    "    #selection fom future states with the existing policy\n",
    "    def cartpole_update(self, observation, action, reward):\n",
    "        old_value = self.q_policy[self._cartpole_obs_index(self.previous_observation), action]\n",
    "        next_actions = self.q_policy[self._cartpole_obs_index(observation)]\n",
    "        next_actions_sum = sum(next_actions)\n",
    "        weighted_actions = [action / next_actions_sum for action in next_actions]\n",
    "        next_action_score = np.random.choice(next_actions, p=weighted_actions)\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_action_score)\n",
    "        self.q_policy[self._cartpole_obs_index(self.previous_observation), action] = new_value\n",
    "\n",
    "    # because cartpole observations are continuous, we have to bucket them and\n",
    "    # calculate an index for the qtable\n",
    "    def _cartpole_obs_index(self, observation):\n",
    "        position, velocity, theta, theta_velocity = observation\n",
    "\n",
    "        bucketed_position = self._bucket(position, CARTPOLE_POSITION_BUCKETS, CARTPOLE_POSITION_RANGE)\n",
    "        bucketed_velocity = self._bucket(velocity, CARTPOLE_VELOCITY_BUCKETS, CARTPOLE_VELOCITY_RANGE)\n",
    "        bucketed_theta = self._bucket(theta, CARTPOLE_THETA_BUCKETS, CARTPOLE_THETA_RANGE)\n",
    "        bucketed_theta_velocity = self._bucket(theta_velocity, CARTPOLE_THETA_VELOCITY_BUCKETS, CARTPOLE_THETA_VELOCITY_RANGE)\n",
    "\n",
    "        position_index = (bucketed_position - 1) * CARTPOLE_VELOCITY_BUCKETS * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        velocity_index = (bucketed_velocity - 1) * CARTPOLE_THETA_BUCKETS * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        theta_index = (bucketed_theta - 1) * CARTPOLE_THETA_VELOCITY_BUCKETS\n",
    "        theta_velocity_index = (bucketed_theta_velocity - 1)\n",
    "\n",
    "        index = position_index + velocity_index + theta_index + theta_velocity_index\n",
    "        return index\n",
    "    \n",
    "    #Method calculates bucket number\n",
    "    def _bucket(self, observation, num_buckets, obs_range):\n",
    "        r_min = obs_range[0]\n",
    "        r_max = obs_range[1]\n",
    "        r_range = r_max - r_min\n",
    "        bucket_size = r_range / num_buckets\n",
    "        bucket = math.ceil((observation + r_range / 2) / bucket_size)\n",
    "\n",
    "        # bound\n",
    "        bucket = min(bucket, num_buckets)\n",
    "        bucket = max(bucket, 1)\n",
    "        return bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a8f4d",
   "metadata": {},
   "source": [
    "### Driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faaadf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driver class houses Random, QLearning, And Temporal Difference For Frozen Lake\n",
    "#Also houses Temporal Difference and Q-learning for Cart-pole\n",
    "class Driver:\n",
    "    def __init__(self, params):\n",
    "        self.epochs = params['epochs']\n",
    "        self.env = params['env']\n",
    "        self.agent = params['agent']\n",
    "        self.training_rewards = []\n",
    "        self.evaluation_rewards = []\n",
    "    \n",
    "    # FrozenLake Random\n",
    "    def run_frozen_lake_random(self):\n",
    "        training_action = lambda _observation: self.agent.action(self.env)\n",
    "        update = lambda _observation, _action, _reward: None\n",
    "        evaluation_action = training_action\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "        \n",
    "    #FrozenLake Q-Learner    \n",
    "    def run_frozen_lake_qlearner(self):\n",
    "        self.agent.initialize_frozen_lake_q_table(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.frozen_lake_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.frozen_lake_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.frozen_lake_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "        \n",
    "    #FrozenLake Temporal Difference calls q-policy\n",
    "    def run_frozen_lake_tdlearner(self):\n",
    "        self.agent.initialize_frozen_lake_q_policy(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.frozen_lake_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.frozen_lake_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.frozen_lake_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "        \n",
    "    #Cart-Pole Temporal Difference calls q-policy\n",
    "    def run_cartpole_tdlearner(self):\n",
    "        self.agent.initialize_cartpole_q_policy(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.cartpole_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.cartpole_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.cartpole_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "        \n",
    "    #Initializes Cart-Pole Q-Learner \n",
    "    def run_cartpole_qlearner(self):\n",
    "        self.agent.initialize_cartpole_q_table(self.env)\n",
    "\n",
    "        training_action = lambda observation: self.agent.cartpole_training_action(self.env, observation)\n",
    "        update = lambda observation, action, reward: self.agent.cartpole_update(observation, action, reward)\n",
    "        evaluation_action = lambda observation: self.agent.cartpole_evaluation_action(observation)\n",
    "\n",
    "        self.run(training_action, update, evaluation_action)\n",
    "        \n",
    "    #Main engine: training, evaluation loop, calls plot\n",
    "    def run(self, training_action, update, evaluation_action):\n",
    "        for i in range(self.epochs):\n",
    "            if ((i + 1) % 1000 == 0):\n",
    "                print(\"progress: {}%\".format(100 * (i + 1) // self.epochs))\n",
    "            self.train_once(training_action, update)\n",
    "            self.evaluate_once(evaluation_action)\n",
    "\n",
    "        self.plot()\n",
    "\n",
    "    #A single instance of training of the agent in the environment\n",
    "    def train_once(self, training_action, update):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = training_action(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "            update(observation, action, reward)\n",
    "        self.training_rewards.append(episode_reward)\n",
    "\n",
    "    #A single instance of evaluation of the agent at it's current level of training\n",
    "    def evaluate_once(self, evaluation_action):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = evaluation_action(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "        self.evaluation_rewards.append(episode_reward)\n",
    "\n",
    "    #Plots training and evaluation reward levels at each epoch\n",
    "    def plot(self):\n",
    "        plt.subplot(211)\n",
    "        plt.plot(self.training_rewards, linewidth=1)\n",
    "        plt.title('Training Rewards over Time')#Updated title depending on which agent/environment combo run\n",
    "        plt.ylabel('Reward')\n",
    "        plt.xlabel('Algorithm Iterations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a8afe",
   "metadata": {},
   "source": [
    "### Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a3a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the Frozen Lake environment\n",
    "#Calls the random agent \n",
    "def frozen_lake_random():\n",
    "    agent = Random()\n",
    "    driver = Driver({\n",
    "        'epochs': 1000,\n",
    "        'env': gym.make('FrozenLake-v1'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_frozen_lake_random()\n",
    "\n",
    "#Creates the Frozen Lake environment\n",
    "#Calls the q-learner agent \n",
    "def frozen_lake_qlearner():\n",
    "    agent = Qlearner({\n",
    "        'alpha': 0.1,\n",
    "        'gamma': 0.6,\n",
    "        'epsilon': 0.3,\n",
    "    })\n",
    "    driver = Driver({\n",
    "        'epochs': 10000,\n",
    "        'env': gym.make('FrozenLake-v1'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_frozen_lake_qlearner()\n",
    "\n",
    "#Creates the Frozen Lake environment\n",
    "#Calls the temporal difference learner agent \n",
    "def frozen_lake_tdlearner():\n",
    "    agent = TDlearner({\n",
    "        'alpha': 0.1,\n",
    "        'gamma': 0.6,\n",
    "        'epsilon': 0.3,\n",
    "    })\n",
    "    driver = Driver({\n",
    "        'epochs': 10000,\n",
    "        'env': gym.make('FrozenLake-v1'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_frozen_lake_tdlearner()\n",
    "\n",
    "#Creates the Cart-pole environment\n",
    "#Calls the temporal difference learner agent \n",
    "def cartpole_tdlearner():\n",
    "    agent = TDlearner({\n",
    "        'alpha': 0.2,\n",
    "        'gamma': 0.5,\n",
    "        'epsilon': 0.1,\n",
    "    })\n",
    "    driver = Driver({\n",
    "        'epochs': 50000,\n",
    "        'env': gym.make('CartPole-v1'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_cartpole_tdlearner()\n",
    "\n",
    "#Creates the Cart-Pole environment\n",
    "#Calls the q-learner agent\n",
    "def cartpole_qlearner():\n",
    "    agent = Qlearner({\n",
    "        'alpha': 0.2,\n",
    "        'gamma': 0.5,\n",
    "        'epsilon': 0.1,\n",
    "    })\n",
    "    driver = Driver({\n",
    "        'epochs': 50000,\n",
    "        'env': gym.make('CartPole-v1'),\n",
    "        'agent': agent,\n",
    "    })\n",
    "    driver.run_cartpole_qlearner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1413c3",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b9976d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 10%\n",
      "progress: 20%\n",
      "progress: 30%\n",
      "progress: 40%\n",
      "progress: 50%\n",
      "progress: 60%\n",
      "progress: 70%\n",
      "progress: 80%\n",
      "progress: 90%\n",
      "progress: 100%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Main Method decides which environment to agent combo to run\n",
    "#*****Uncomment the desired one\n",
    "#Alter plot title to suit respective environment in Driver class\n",
    "if __name__ == '__main__':\n",
    "    #frozen_lake_random()\n",
    "    frozen_lake_qlearner()\n",
    "    #frozen_lake_tdlearner()\n",
    "    #cartpole_tdlearner()\n",
    "    #cartpole_qlearner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4486f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
